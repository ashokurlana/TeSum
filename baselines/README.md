## Benchmark Models for Telugu Abstractive Summarization

#### Pointer_Generator : [ Get To The Point: Summarization with Pointer-Generator Networks ](https://arxiv.org/pdf/1704.04368.pdf)
#### ML_RL: [A Deep Reinforced Model For Abstractive Summarization ](https://arxiv.org/pdf/1705.04304.pdf)
#### BERTSum : [Text Summarization with Pretrained Encoders](https://arxiv.org/pdf/1908.08345.pdf)
#### mT5 : [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/pdf/2010.11934.pdf)
#### mBART50 : [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/pdf/2008.00401.pdf)
#### IndicBART : [IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages](https://arxiv.org/pdf/2109.02903.pdf)
#### Adapters: [AdapterHub: A Framework for Adapting Transformers](https://aclanthology.org/2020.emnlp-demos.7.pdf)

You can see the individual directories for above mentioned models. Go to the respective model directory and follow the corresponding instructions to setup the models.

### Trained Model

You can download the mBART-large50 fine-tuned on TeSum dataset. Our model publicly available on huggingface.

[mBART-TeSum](https://huggingface.co/ashokurlana/mBART-TeSum)
